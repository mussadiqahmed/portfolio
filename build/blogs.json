{"status":"ok","feed":{"url":"https://medium.com/feed/@mussadiqahmed90","title":"Stories by Muhammad Ahmed on Medium","link":"https://medium.com/@mussadiqahmed90?source=rss-01d818814266------2","author":"","description":"Stories by Muhammad Ahmed on Medium","image":"https://cdn-images-1.medium.com/fit/c/150/150/1*vuadfEUq_j05mL9PdXVIFw.jpeg"},"items":[{"title":"Building a Smart Medical Assistant with RAG and Streamlit","pubDate":"2025-04-07 18:36:47","link":"https://medium.com/@mussadiqahmed90/building-a-smart-medical-assistant-with-rag-and-streamlit-40595ee7d767?source=rss-01d818814266------2","guid":"https://medium.com/p/40595ee7d767","author":"Muhammad Ahmed","thumbnail":"","description":"\n<p><strong>Introduction:</strong> For my latest GenAI assignment, I built a clinical diagnosis assistant that can answer medical questions using real hospital data. I used something called RAG (Retrieval-Augmented Generation), which is basically a smart way to make AI give better answers by giving it real facts to read\u00a0first.</p>\n<h3>\ud83e\udde0 What is\u00a0RAG?</h3>\n<p>RAG is a technique where we mix two\u00a0parts:</p>\n<ol>\n<li>A <strong>retriever</strong>: It finds relevant documents from a\u00a0dataset.</li>\n<li>A <strong>generator</strong>: It takes those documents and writes a useful, understandable answer.</li>\n</ol>\n<h3>\ud83d\udcca The Dataset: MIMIC-IV-Ext Direct</h3>\n<p>This dataset contains clinical notes from ICU patients. I explored it to understand the structure and cleaned it\u00a0by:</p>\n<ul>\n<li>Removing missing\u00a0entries</li>\n<li>Tokenizing the\u00a0text</li>\n<li>Creating embeddings (to allow smart searching)</li>\n</ul>\n<h3>\ud83d\udd0d Building the Retriever</h3>\n<p>I tested two\u00a0methods:</p>\n<ul>\n<li>\n<strong>BM25</strong>: A keyword-based method</li>\n<li>\n<strong>Dense retrieval with embeddings</strong>: Better for meaning-based searches</li>\n</ul>\n<p>I ended up using <strong>FAISS + Sentence Transformers</strong>, which made retrieval fast and meaningful.</p>\n<h3>\ud83e\udd16 The Generator (Groq\u00a0LLM)</h3>\n<p>For the answer generation, I used <strong>Groq\u2019s Mixtral model</strong> through langchain-groq. I passed the retrieved documents into the LLM using a custom prompt\u00a0like:</p>\n<blockquote><em>\u201cBased on the following clinical notes, what is the most likely diagnosis?\u201d</em></blockquote>\n<h3>\ud83d\udcbb Streamlit Frontend</h3>\n<p>To make everything interactive, I built a Streamlit app\u00a0that:</p>\n<ul>\n<li>Lets users enter medical\u00a0queries</li>\n<li>Shows the documents used</li>\n<li>Displays the AI\u2019s\u00a0answer</li>\n</ul>\n<h3>\ud83d\udcc8 Evaluation &amp;\u00a0Results</h3>\n<p>I used precision, recall (for retrieval), and accuracy/coherence (for generation). I\u00a0found:</p>\n<ul>\n<li>Retrieval quality strongly affects\u00a0output</li>\n<li>Prompting with full notes gave better\u00a0results</li>\n</ul>\n<h3>\u2696\ufe0f Ethical\u00a0Thoughts</h3>\n<p>Even though the data is de-identified, I avoided showing sensitive details. In real life, privacy would be a top priority.</p>\n<h3>\ud83d\ude4c What I\u00a0Learned</h3>\n<ul>\n<li>Retrieval and generation need to be in\u00a0sync</li>\n<li>Streamlit is super helpful for quick frontends</li>\n<li>Groq LLMs are fast and\u00a0accurate</li>\n</ul>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=40595ee7d767\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<p><strong>Introduction:</strong> For my latest GenAI assignment, I built a clinical diagnosis assistant that can answer medical questions using real hospital data. I used something called RAG (Retrieval-Augmented Generation), which is basically a smart way to make AI give better answers by giving it real facts to read\u00a0first.</p>\n<h3>\ud83e\udde0 What is\u00a0RAG?</h3>\n<p>RAG is a technique where we mix two\u00a0parts:</p>\n<ol>\n<li>A <strong>retriever</strong>: It finds relevant documents from a\u00a0dataset.</li>\n<li>A <strong>generator</strong>: It takes those documents and writes a useful, understandable answer.</li>\n</ol>\n<h3>\ud83d\udcca The Dataset: MIMIC-IV-Ext Direct</h3>\n<p>This dataset contains clinical notes from ICU patients. I explored it to understand the structure and cleaned it\u00a0by:</p>\n<ul>\n<li>Removing missing\u00a0entries</li>\n<li>Tokenizing the\u00a0text</li>\n<li>Creating embeddings (to allow smart searching)</li>\n</ul>\n<h3>\ud83d\udd0d Building the Retriever</h3>\n<p>I tested two\u00a0methods:</p>\n<ul>\n<li>\n<strong>BM25</strong>: A keyword-based method</li>\n<li>\n<strong>Dense retrieval with embeddings</strong>: Better for meaning-based searches</li>\n</ul>\n<p>I ended up using <strong>FAISS + Sentence Transformers</strong>, which made retrieval fast and meaningful.</p>\n<h3>\ud83e\udd16 The Generator (Groq\u00a0LLM)</h3>\n<p>For the answer generation, I used <strong>Groq\u2019s Mixtral model</strong> through langchain-groq. I passed the retrieved documents into the LLM using a custom prompt\u00a0like:</p>\n<blockquote><em>\u201cBased on the following clinical notes, what is the most likely diagnosis?\u201d</em></blockquote>\n<h3>\ud83d\udcbb Streamlit Frontend</h3>\n<p>To make everything interactive, I built a Streamlit app\u00a0that:</p>\n<ul>\n<li>Lets users enter medical\u00a0queries</li>\n<li>Shows the documents used</li>\n<li>Displays the AI\u2019s\u00a0answer</li>\n</ul>\n<h3>\ud83d\udcc8 Evaluation &amp;\u00a0Results</h3>\n<p>I used precision, recall (for retrieval), and accuracy/coherence (for generation). I\u00a0found:</p>\n<ul>\n<li>Retrieval quality strongly affects\u00a0output</li>\n<li>Prompting with full notes gave better\u00a0results</li>\n</ul>\n<h3>\u2696\ufe0f Ethical\u00a0Thoughts</h3>\n<p>Even though the data is de-identified, I avoided showing sensitive details. In real life, privacy would be a top priority.</p>\n<h3>\ud83d\ude4c What I\u00a0Learned</h3>\n<ul>\n<li>Retrieval and generation need to be in\u00a0sync</li>\n<li>Streamlit is super helpful for quick frontends</li>\n<li>Groq LLMs are fast and\u00a0accurate</li>\n</ul>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=40595ee7d767\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":[]},{"title":"Fine-Tuning AI for Math Creativity and Accuracy","pubDate":"2025-03-14 18:22:08","link":"https://medium.com/@mussadiqahmed90/fine-tuning-ai-for-math-creativity-and-accuracy-57e8f4f0be61?source=rss-01d818814266------2","guid":"https://medium.com/p/57e8f4f0be61","author":"Muhammad Ahmed","thumbnail":"","description":"\n<h3>Introduction</h3>\n<p>Artificial Intelligence (AI) has significantly evolved in creative and problem-solving tasks. In this blog, we explore three unique AI fine-tuning challenges that blend math, humor, and logical accuracy. These include generating math riddles, correcting viral math memes, and solving emoji-based math problems.</p>\n<h3>1. Math Riddle\u00a0Factory</h3>\n<h3>Goal</h3>\n<p>Fine-tune a language model to generate engaging math riddles with logical solutions.</p>\n<h3>Steps</h3>\n<p><strong>Dataset Creation</strong></p>\n<ul><li>We curated 30 simple math riddles and their solutions. Example:<br> \u201cWhat number becomes zero when you subtract 15 from half of it?\u201d (Answer:\u00a030).</li></ul>\n<p><strong>Fine-Tuning the\u00a0Model</strong></p>\n<ul><li>We used GPT-2 or TinyLlama and fine-tuned it on our dataset for 2\u20133\u00a0epochs.</li></ul>\n<p><strong>Testing</strong></p>\n<ul><li>We generated five riddles and validated their correctness.</li></ul>\n<h3>Results</h3>\n<p>Here are three of our best riddles along with their solutions:</p>\n<ul>\n<li>\n<strong>Riddle 1:</strong> \u201cI am a number. Double me and subtract 10 to get 14. What am\u00a0I?\u201d</li>\n<li>Solution: (14 + 10) / 2 =\u00a012</li>\n<li>\n<strong>Riddle 2:</strong> \u201cWhat comes next? 2, 6, 12, 20,\u00a0?\u201d</li>\n<li>Solution: The pattern follows n(n+1)n(n+1), so the next number is\u00a030.</li>\n<li>\n<strong>Riddle 3:</strong> \u201cI am a three-digit number. My tens digit is five more than my ones digit, and my hundreds digit is eight less than my tens digit. What am\u00a0I?\u201d</li>\n<li>Solution: 194</li>\n</ul>\n<h3>2. Math Meme\u00a0Repair</h3>\n<h3>Goal</h3>\n<p>Correct viral math memes that contain incorrect calculations or\u00a0logic.</p>\n<h3>Steps</h3>\n<p><strong>Dataset Collection</strong></p>\n<ul><li>We gathered 20 incorrect math memes, such as misinterpretations of PEMDAS or division\u00a0errors.</li></ul>\n<p><strong>Fine-Tuning the\u00a0Model</strong></p>\n<ul><li>We trained an AI model to map incorrect equations to corrected solutions.</li></ul>\n<p><strong>Testing</strong></p>\n<ul><li>We tested three new incorrect memes to check if the model correctly identified and explained errors.</li></ul>\n<h3>Results</h3>\n<ul>\n<li><strong>Example 1:</strong></li>\n<li>Incorrect: \u201c8 \u00f7 2(2+2) =\u00a01?\u201d</li>\n<li>Correct Answer: Applying PEMDAS, 8 \u00f7 2(4) = 8 \u00f7 2 \u00d7 4 =\u00a016.</li>\n<li>Model Explanation: Division and multiplication are solved from left to\u00a0right.</li>\n<li><strong>Example 2:</strong></li>\n<li>Incorrect: \u201cWrong: \u2075\u00b2 =\u00a010\u201d</li>\n<li>Correct Answer: \u2075\u00b2 =\u00a025</li>\n<li>Model\u2019s Funny Error Rating: \u201c90% sass, 10% patience!\u201d</li>\n</ul>\n<h3>3. Creative Math Problem Solver: Emoji\u00a0Math</h3>\n<h3>Goal</h3>\n<p>Develop an AI to solve math problems written in\u00a0emojis.</p>\n<h3>Steps</h3>\n<p><strong>Dataset Creation</strong></p>\n<ul>\n<li>We created a dataset of 30 emoji-based equations. Example:</li>\n<li>\u201c\u26bd + \u26bd + \u26bd = 12 \u2192 \u26bd =\u00a04\u201d</li>\n</ul>\n<p><strong>Fine-Tuning</strong></p>\n<ul><li>We trained the model to recognize and solve emoji-based equations.</li></ul>\n<p><strong>Testing</strong></p>\n<ul><li>The model solved three new emoji problems.</li></ul>\n<h3>Results</h3>\n<ul>\n<li>\n<strong>Problem 1:</strong> \u26bd + \u26bd + \u26bd = 12 \u2192 \u26bd =\u00a04</li>\n<li>\n<strong>Problem 2:</strong> \u2708\ufe0f + \u2708\ufe0f + \u2708\ufe0f + \u2708\ufe0f = 20 \u2192 \u2708\ufe0f =\u00a05</li>\n<li>\n<strong>Problem 3:</strong> \ud83c\udf4f + \ud83c\udf4f + \ud83c\udf4e = 10, \ud83c\udf4e = 4 \u2192 \ud83c\udf4f =\u00a03</li>\n</ul>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=57e8f4f0be61\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<h3>Introduction</h3>\n<p>Artificial Intelligence (AI) has significantly evolved in creative and problem-solving tasks. In this blog, we explore three unique AI fine-tuning challenges that blend math, humor, and logical accuracy. These include generating math riddles, correcting viral math memes, and solving emoji-based math problems.</p>\n<h3>1. Math Riddle\u00a0Factory</h3>\n<h3>Goal</h3>\n<p>Fine-tune a language model to generate engaging math riddles with logical solutions.</p>\n<h3>Steps</h3>\n<p><strong>Dataset Creation</strong></p>\n<ul><li>We curated 30 simple math riddles and their solutions. Example:<br> \u201cWhat number becomes zero when you subtract 15 from half of it?\u201d (Answer:\u00a030).</li></ul>\n<p><strong>Fine-Tuning the\u00a0Model</strong></p>\n<ul><li>We used GPT-2 or TinyLlama and fine-tuned it on our dataset for 2\u20133\u00a0epochs.</li></ul>\n<p><strong>Testing</strong></p>\n<ul><li>We generated five riddles and validated their correctness.</li></ul>\n<h3>Results</h3>\n<p>Here are three of our best riddles along with their solutions:</p>\n<ul>\n<li>\n<strong>Riddle 1:</strong> \u201cI am a number. Double me and subtract 10 to get 14. What am\u00a0I?\u201d</li>\n<li>Solution: (14 + 10) / 2 =\u00a012</li>\n<li>\n<strong>Riddle 2:</strong> \u201cWhat comes next? 2, 6, 12, 20,\u00a0?\u201d</li>\n<li>Solution: The pattern follows n(n+1)n(n+1), so the next number is\u00a030.</li>\n<li>\n<strong>Riddle 3:</strong> \u201cI am a three-digit number. My tens digit is five more than my ones digit, and my hundreds digit is eight less than my tens digit. What am\u00a0I?\u201d</li>\n<li>Solution: 194</li>\n</ul>\n<h3>2. Math Meme\u00a0Repair</h3>\n<h3>Goal</h3>\n<p>Correct viral math memes that contain incorrect calculations or\u00a0logic.</p>\n<h3>Steps</h3>\n<p><strong>Dataset Collection</strong></p>\n<ul><li>We gathered 20 incorrect math memes, such as misinterpretations of PEMDAS or division\u00a0errors.</li></ul>\n<p><strong>Fine-Tuning the\u00a0Model</strong></p>\n<ul><li>We trained an AI model to map incorrect equations to corrected solutions.</li></ul>\n<p><strong>Testing</strong></p>\n<ul><li>We tested three new incorrect memes to check if the model correctly identified and explained errors.</li></ul>\n<h3>Results</h3>\n<ul>\n<li><strong>Example 1:</strong></li>\n<li>Incorrect: \u201c8 \u00f7 2(2+2) =\u00a01?\u201d</li>\n<li>Correct Answer: Applying PEMDAS, 8 \u00f7 2(4) = 8 \u00f7 2 \u00d7 4 =\u00a016.</li>\n<li>Model Explanation: Division and multiplication are solved from left to\u00a0right.</li>\n<li><strong>Example 2:</strong></li>\n<li>Incorrect: \u201cWrong: \u2075\u00b2 =\u00a010\u201d</li>\n<li>Correct Answer: \u2075\u00b2 =\u00a025</li>\n<li>Model\u2019s Funny Error Rating: \u201c90% sass, 10% patience!\u201d</li>\n</ul>\n<h3>3. Creative Math Problem Solver: Emoji\u00a0Math</h3>\n<h3>Goal</h3>\n<p>Develop an AI to solve math problems written in\u00a0emojis.</p>\n<h3>Steps</h3>\n<p><strong>Dataset Creation</strong></p>\n<ul>\n<li>We created a dataset of 30 emoji-based equations. Example:</li>\n<li>\u201c\u26bd + \u26bd + \u26bd = 12 \u2192 \u26bd =\u00a04\u201d</li>\n</ul>\n<p><strong>Fine-Tuning</strong></p>\n<ul><li>We trained the model to recognize and solve emoji-based equations.</li></ul>\n<p><strong>Testing</strong></p>\n<ul><li>The model solved three new emoji problems.</li></ul>\n<h3>Results</h3>\n<ul>\n<li>\n<strong>Problem 1:</strong> \u26bd + \u26bd + \u26bd = 12 \u2192 \u26bd =\u00a04</li>\n<li>\n<strong>Problem 2:</strong> \u2708\ufe0f + \u2708\ufe0f + \u2708\ufe0f + \u2708\ufe0f = 20 \u2192 \u2708\ufe0f =\u00a05</li>\n<li>\n<strong>Problem 3:</strong> \ud83c\udf4f + \ud83c\udf4f + \ud83c\udf4e = 10, \ud83c\udf4e = 4 \u2192 \ud83c\udf4f =\u00a03</li>\n</ul>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=57e8f4f0be61\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":[]},{"title":"Building Transformer-based Seq2Seq Models for Code Conversion and Neural Machine Translation","pubDate":"2025-02-27 14:26:16","link":"https://medium.com/@mussadiqahmed90/building-transformer-based-seq2seq-models-for-code-conversion-and-neural-machine-translation-31f6c2b9fd02?source=rss-01d818814266------2","guid":"https://medium.com/p/31f6c2b9fd02","author":"Muhammad Ahmed","thumbnail":"","description":"\n<h3>Introduction</h3>\n<p>With the rapid advancements in Generative AI, Transformer-based models have gained significant traction in various applications, including natural language processing (NLP), code generation, and machine translation. In this blog, I\u2019ll walk you through my journey of implementing three different Transformer-based Sequence-to-Sequence (Seq2Seq) models from\u00a0scratch:</p>\n<ol>\n<li><strong>Pseudocode to C++ Code Conversion</strong></li>\n<li><strong>C++ Code to Pseudocode Conversion</strong></li>\n<li><strong>Neural Machine Translation (NMT) from Arabic to\u00a0English</strong></li>\n</ol>\n<p>Each of these models was built and deployed using <strong>Streamlit</strong>, making them accessible via a simple web interface. Let\u2019s dive into the\u00a0details!</p>\n<h3>1. Pseudocode to C++ Code Conversion</h3>\n<h3>Problem Statement</h3>\n<p>Understanding pseudocode is essential for software engineers and computer science students. Automating the conversion of pseudocode into C++ can help bridge the gap between theoretical logic and implementation.</p>\n<h3>Approach</h3>\n<ul>\n<li>\n<strong>Data Preparation</strong>: Collected and cleaned a dataset containing paired examples of pseudocode and corresponding C++ implementations.</li>\n<li>\n<strong>Model Architecture</strong>: Implemented a Transformer-based Seq2Seq model with an encoder-decoder architecture.</li>\n<li>\n<strong>Training</strong>: Trained the model on labeled data using a cross-entropy loss function and the Adam optimizer.</li>\n<li>\n<strong>Deployment</strong>: Created a Streamlit app where users can input pseudocode and get the equivalent C++\u00a0code.</li>\n</ul>\n<h3>Key Takeaways</h3>\n<ul>\n<li>Handling syntax variations in pseudocode was a challenge.</li>\n<li>Training on a well-curated dataset significantly improved model performance.</li>\n</ul>\n<h3>2. C++ Code to Pseudocode Conversion</h3>\n<h3>Problem Statement</h3>\n<p>While coding skills are crucial, understanding and explaining logic in plain language is equally important. This model reverses the first task by converting C++ code into readable pseudocode.</p>\n<h3>Approach</h3>\n<ul>\n<li>\n<strong>Dataset</strong>: Used a dataset containing C++ functions and their corresponding pseudocode descriptions.</li>\n<li>\n<strong>Model Training</strong>: Adapted the previous Transformer architecture to translate structured code into human-readable steps.</li>\n<li>\n<strong>Web App</strong>: Built an interactive Streamlit UI where users can input C++ code and get the pseudocode explanation.</li>\n</ul>\n<h3>Challenges and Learnings</h3>\n<ul>\n<li>Extracting meaningful variable names and function descriptions was challenging.</li>\n<li>Using a Transformer model instead of simple rule-based parsing proved beneficial for handling complex\u00a0logic.</li>\n</ul>\n<h3>3. Neural Machine Translation (NMT): Arabic to\u00a0English</h3>\n<h3>Problem Statement</h3>\n<p>Machine translation is a core application of NLP, and translating from Arabic to English poses unique challenges due to the differences in syntax, grammar, and morphology.</p>\n<h3>Approach</h3>\n<ul>\n<li>\n<strong>Dataset</strong>: Used the datasets library to fetch a large Arabic-English parallel\u00a0corpus.</li>\n<li>\n<strong>Model</strong>: Implemented a Transformer-based Seq2Seq model trained with positional encoding and attention mechanisms.</li>\n<li>\n<strong>Fine-tuning</strong>: Optimized hyperparameters to improve translation accuracy.</li>\n<li>\n<strong>Deployment</strong>: Integrated the model into a Streamlit app where users can input Arabic text and receive an English translation.</li>\n</ul>\n<h3>Observations</h3>\n<ul>\n<li>Attention mechanisms helped significantly in handling long sequences.</li>\n<li>More training data improved contextual understanding.</li>\n</ul>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=31f6c2b9fd02\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<h3>Introduction</h3>\n<p>With the rapid advancements in Generative AI, Transformer-based models have gained significant traction in various applications, including natural language processing (NLP), code generation, and machine translation. In this blog, I\u2019ll walk you through my journey of implementing three different Transformer-based Sequence-to-Sequence (Seq2Seq) models from\u00a0scratch:</p>\n<ol>\n<li><strong>Pseudocode to C++ Code Conversion</strong></li>\n<li><strong>C++ Code to Pseudocode Conversion</strong></li>\n<li><strong>Neural Machine Translation (NMT) from Arabic to\u00a0English</strong></li>\n</ol>\n<p>Each of these models was built and deployed using <strong>Streamlit</strong>, making them accessible via a simple web interface. Let\u2019s dive into the\u00a0details!</p>\n<h3>1. Pseudocode to C++ Code Conversion</h3>\n<h3>Problem Statement</h3>\n<p>Understanding pseudocode is essential for software engineers and computer science students. Automating the conversion of pseudocode into C++ can help bridge the gap between theoretical logic and implementation.</p>\n<h3>Approach</h3>\n<ul>\n<li>\n<strong>Data Preparation</strong>: Collected and cleaned a dataset containing paired examples of pseudocode and corresponding C++ implementations.</li>\n<li>\n<strong>Model Architecture</strong>: Implemented a Transformer-based Seq2Seq model with an encoder-decoder architecture.</li>\n<li>\n<strong>Training</strong>: Trained the model on labeled data using a cross-entropy loss function and the Adam optimizer.</li>\n<li>\n<strong>Deployment</strong>: Created a Streamlit app where users can input pseudocode and get the equivalent C++\u00a0code.</li>\n</ul>\n<h3>Key Takeaways</h3>\n<ul>\n<li>Handling syntax variations in pseudocode was a challenge.</li>\n<li>Training on a well-curated dataset significantly improved model performance.</li>\n</ul>\n<h3>2. C++ Code to Pseudocode Conversion</h3>\n<h3>Problem Statement</h3>\n<p>While coding skills are crucial, understanding and explaining logic in plain language is equally important. This model reverses the first task by converting C++ code into readable pseudocode.</p>\n<h3>Approach</h3>\n<ul>\n<li>\n<strong>Dataset</strong>: Used a dataset containing C++ functions and their corresponding pseudocode descriptions.</li>\n<li>\n<strong>Model Training</strong>: Adapted the previous Transformer architecture to translate structured code into human-readable steps.</li>\n<li>\n<strong>Web App</strong>: Built an interactive Streamlit UI where users can input C++ code and get the pseudocode explanation.</li>\n</ul>\n<h3>Challenges and Learnings</h3>\n<ul>\n<li>Extracting meaningful variable names and function descriptions was challenging.</li>\n<li>Using a Transformer model instead of simple rule-based parsing proved beneficial for handling complex\u00a0logic.</li>\n</ul>\n<h3>3. Neural Machine Translation (NMT): Arabic to\u00a0English</h3>\n<h3>Problem Statement</h3>\n<p>Machine translation is a core application of NLP, and translating from Arabic to English poses unique challenges due to the differences in syntax, grammar, and morphology.</p>\n<h3>Approach</h3>\n<ul>\n<li>\n<strong>Dataset</strong>: Used the datasets library to fetch a large Arabic-English parallel\u00a0corpus.</li>\n<li>\n<strong>Model</strong>: Implemented a Transformer-based Seq2Seq model trained with positional encoding and attention mechanisms.</li>\n<li>\n<strong>Fine-tuning</strong>: Optimized hyperparameters to improve translation accuracy.</li>\n<li>\n<strong>Deployment</strong>: Integrated the model into a Streamlit app where users can input Arabic text and receive an English translation.</li>\n</ul>\n<h3>Observations</h3>\n<ul>\n<li>Attention mechanisms helped significantly in handling long sequences.</li>\n<li>More training data improved contextual understanding.</li>\n</ul>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=31f6c2b9fd02\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":[]},{"title":"Generating Roman Urdu Poetry with LSTMs","pubDate":"2025-02-17 18:29:36","link":"https://medium.com/@mussadiqahmed90/generating-roman-urdu-poetry-with-lstms-47f1c4b7c770?source=rss-01d818814266------2","guid":"https://medium.com/p/47f1c4b7c770","author":"Muhammad Ahmed","thumbnail":"","description":"\n<h3>Introduction</h3>\n<p>Poetry has always been a medium of human expression, carrying deep emotions and cultural richness. With the rise of AI, we can now teach machines to generate poetry that mimics human creativity. In this blog, we\u2019ll explore how to train an LSTM (Long Short-Term Memory) model on Roman Urdu poetry and deploy it using Gradio for easy\u00a0access.</p>\n<h3>Why Roman\u00a0Urdu?</h3>\n<p>Roman Urdu is a unique script used by many Urdu speakers who type Urdu using the Latin alphabet, especially on social media and messaging platforms. Training an AI model on Roman Urdu poetry helps preserve this style and makes poetry generation accessible to a broader audience.</p>\n<h3>Overview of Our\u00a0Approach</h3>\n<ol>\n<li>\n<strong>Dataset Preparation</strong>\u200a\u2014\u200aCollecting and preprocessing Roman Urdu\u00a0poetry.</li>\n<li>\n<strong>LSTM Model Training</strong>\u200a\u2014\u200aTraining an LSTM-based neural network for text generation.</li>\n<li>\n<strong>Sampling Poetry</strong>\u200a\u2014\u200aUsing the trained model to generate Roman Urdu\u00a0verses.</li>\n<li>\n<strong>Deployment with Gradio</strong>\u200a\u2014\u200aCreating an interactive UI for users to generate\u00a0poetry.</li>\n</ol>\n<h3>Step 1: Preparing the\u00a0Dataset</h3>\n<p>The first step is to gather a dataset of Roman Urdu poetry. We can collect poetry from open-source repositories, websites, or manually curate a list of\u00a0verses.</p>\n<h3>Preprocessing the\u00a0Text</h3>\n<ul>\n<li>Convert all text to lowercase.</li>\n<li>Remove unnecessary punctuation.</li>\n<li>Tokenize text into sequences.</li>\n<li>Create input-output pairs for training (each sequence of words predicts the next\u00a0word).</li>\n</ul>\n<pre>import numpy as np<br>import tensorflow as tf<br>from tensorflow.keras.preprocessing.text import Tokenizer<br>from tensorflow.keras.preprocessing.sequence import pad_sequences<br># Sample dataset<br>poetry = [<br>    \"ishq me hum tum nahi,\",<br>    \"dil ka dard kisi ne suna nahi,\",<br>    \"chandni raaton me yaad aye tumhari\",<br>]</pre>\n<pre># Tokenization<br>tokenizer = Tokenizer()<br>tokenizer.fit_on_texts(poetry)<br>total_words = len(tokenizer.word_index) + 1<br>sequences = []</pre>\n<pre>for line in poetry:<br>    token_list = tokenizer.texts_to_sequences([line])[0]<br>    for i in range(1, len(token_list)):<br>        n_gram_sequence = token_list[:i+1]<br>        sequences.append(n_gram_sequence)</pre>\n<pre># Padding<br>max_seq_length = max(len(seq) for seq in sequences)<br>sequences = pad_sequences(sequences, maxlen=max_seq_length, padding='pre')<br>X, y = sequences[:, :-1], sequences[:, -1]<br>y = tf.keras.utils.to_categorical(y, num_classes=total_words)</pre>\n<h3>Step 2: Training the LSTM\u00a0Model</h3>\n<p>Now, we define and train an LSTM model to predict the next word in a poetry sequence.</p>\n<pre>from tensorflow.keras.models import Sequential<br>from tensorflow.keras.layers import LSTM, Embedding, Dense</pre>\n<pre># Model definition<br>model = Sequential([<br>    Embedding(total_words, 100, input_length=max_seq_length-1),<br>    LSTM(100, return_sequences=True),<br>    LSTM(100),<br>    Dense(total_words, activation='softmax')<br>])</pre>\n<pre># Compile and train<br>model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])<br>model.fit(X, y, epochs=100, verbose=1)</pre>\n<h3>Step 3: Generating Poetry</h3>\n<p>Once trained,\u00a0mak</p>\n<pre>def generate_poetry(seed_text, next_words=5):<br>    for _ in range(next_words):<br>        token_list = tokenizer.texts_to_sequences([seed_text])[0]<br>        token_list = pad_sequences([token_list], maxlen=max_seq_length-1, padding='pre')<br>        predicted = np.argmax(model.predict(token_list), axis=-1)<br>        output_word = \"\"<br>        for word, index in tokenizer.word_index.items():<br>            if index == predicted:<br>                output_word = word<br>                break<br>        seed_text += \" \" + output_word<br>    return seed_text</pre>\n<h3>Step 4: Deploying with\u00a0Gradio</h3>\n<p>To make the model accessible, we use Gradio to create a simple web interface where users can input a starting phrase and receive generated poetry.</p>\n<pre>import gradio as gr</pre>\n<pre>def poetry_interface(text):<br>    return generate_poetry(text)</pre>\n<pre>iface = gr.Interface(fn=poetry_interface, inputs=\"text\", outputs=\"text\")<br>iface.launch()</pre>\n<p>Now, running the script will start a web app where users can enter a seed phrase and get AI-generated poetry in Roman\u00a0Urdu.</p>\n<h3>Conclusion</h3>\n<p>In this blog, we covered how to train an LSTM on Roman Urdu poetry and deploy it using Gradio. This project showcases how AI can contribute to the preservation and creativity of linguistic art\u00a0forms.</p>\n<p>Want to try it yourself? Train your own LSTM on a larger dataset and experiment with different architectures for even better\u00a0results!</p>\n<p>What\u2019s next? How about fine-tuning the model with attention mechanisms or training on user-generated poetry submissions? The possibilities are\u00a0endless!</p>\n<p>Let me know your thoughts in the comments. Happy coding!\u00a0\ud83d\ude80</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=47f1c4b7c770\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<h3>Introduction</h3>\n<p>Poetry has always been a medium of human expression, carrying deep emotions and cultural richness. With the rise of AI, we can now teach machines to generate poetry that mimics human creativity. In this blog, we\u2019ll explore how to train an LSTM (Long Short-Term Memory) model on Roman Urdu poetry and deploy it using Gradio for easy\u00a0access.</p>\n<h3>Why Roman\u00a0Urdu?</h3>\n<p>Roman Urdu is a unique script used by many Urdu speakers who type Urdu using the Latin alphabet, especially on social media and messaging platforms. Training an AI model on Roman Urdu poetry helps preserve this style and makes poetry generation accessible to a broader audience.</p>\n<h3>Overview of Our\u00a0Approach</h3>\n<ol>\n<li>\n<strong>Dataset Preparation</strong>\u200a\u2014\u200aCollecting and preprocessing Roman Urdu\u00a0poetry.</li>\n<li>\n<strong>LSTM Model Training</strong>\u200a\u2014\u200aTraining an LSTM-based neural network for text generation.</li>\n<li>\n<strong>Sampling Poetry</strong>\u200a\u2014\u200aUsing the trained model to generate Roman Urdu\u00a0verses.</li>\n<li>\n<strong>Deployment with Gradio</strong>\u200a\u2014\u200aCreating an interactive UI for users to generate\u00a0poetry.</li>\n</ol>\n<h3>Step 1: Preparing the\u00a0Dataset</h3>\n<p>The first step is to gather a dataset of Roman Urdu poetry. We can collect poetry from open-source repositories, websites, or manually curate a list of\u00a0verses.</p>\n<h3>Preprocessing the\u00a0Text</h3>\n<ul>\n<li>Convert all text to lowercase.</li>\n<li>Remove unnecessary punctuation.</li>\n<li>Tokenize text into sequences.</li>\n<li>Create input-output pairs for training (each sequence of words predicts the next\u00a0word).</li>\n</ul>\n<pre>import numpy as np<br>import tensorflow as tf<br>from tensorflow.keras.preprocessing.text import Tokenizer<br>from tensorflow.keras.preprocessing.sequence import pad_sequences<br># Sample dataset<br>poetry = [<br>    \"ishq me hum tum nahi,\",<br>    \"dil ka dard kisi ne suna nahi,\",<br>    \"chandni raaton me yaad aye tumhari\",<br>]</pre>\n<pre># Tokenization<br>tokenizer = Tokenizer()<br>tokenizer.fit_on_texts(poetry)<br>total_words = len(tokenizer.word_index) + 1<br>sequences = []</pre>\n<pre>for line in poetry:<br>    token_list = tokenizer.texts_to_sequences([line])[0]<br>    for i in range(1, len(token_list)):<br>        n_gram_sequence = token_list[:i+1]<br>        sequences.append(n_gram_sequence)</pre>\n<pre># Padding<br>max_seq_length = max(len(seq) for seq in sequences)<br>sequences = pad_sequences(sequences, maxlen=max_seq_length, padding='pre')<br>X, y = sequences[:, :-1], sequences[:, -1]<br>y = tf.keras.utils.to_categorical(y, num_classes=total_words)</pre>\n<h3>Step 2: Training the LSTM\u00a0Model</h3>\n<p>Now, we define and train an LSTM model to predict the next word in a poetry sequence.</p>\n<pre>from tensorflow.keras.models import Sequential<br>from tensorflow.keras.layers import LSTM, Embedding, Dense</pre>\n<pre># Model definition<br>model = Sequential([<br>    Embedding(total_words, 100, input_length=max_seq_length-1),<br>    LSTM(100, return_sequences=True),<br>    LSTM(100),<br>    Dense(total_words, activation='softmax')<br>])</pre>\n<pre># Compile and train<br>model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])<br>model.fit(X, y, epochs=100, verbose=1)</pre>\n<h3>Step 3: Generating Poetry</h3>\n<p>Once trained,\u00a0mak</p>\n<pre>def generate_poetry(seed_text, next_words=5):<br>    for _ in range(next_words):<br>        token_list = tokenizer.texts_to_sequences([seed_text])[0]<br>        token_list = pad_sequences([token_list], maxlen=max_seq_length-1, padding='pre')<br>        predicted = np.argmax(model.predict(token_list), axis=-1)<br>        output_word = \"\"<br>        for word, index in tokenizer.word_index.items():<br>            if index == predicted:<br>                output_word = word<br>                break<br>        seed_text += \" \" + output_word<br>    return seed_text</pre>\n<h3>Step 4: Deploying with\u00a0Gradio</h3>\n<p>To make the model accessible, we use Gradio to create a simple web interface where users can input a starting phrase and receive generated poetry.</p>\n<pre>import gradio as gr</pre>\n<pre>def poetry_interface(text):<br>    return generate_poetry(text)</pre>\n<pre>iface = gr.Interface(fn=poetry_interface, inputs=\"text\", outputs=\"text\")<br>iface.launch()</pre>\n<p>Now, running the script will start a web app where users can enter a seed phrase and get AI-generated poetry in Roman\u00a0Urdu.</p>\n<h3>Conclusion</h3>\n<p>In this blog, we covered how to train an LSTM on Roman Urdu poetry and deploy it using Gradio. This project showcases how AI can contribute to the preservation and creativity of linguistic art\u00a0forms.</p>\n<p>Want to try it yourself? Train your own LSTM on a larger dataset and experiment with different architectures for even better\u00a0results!</p>\n<p>What\u2019s next? How about fine-tuning the model with attention mechanisms or training on user-generated poetry submissions? The possibilities are\u00a0endless!</p>\n<p>Let me know your thoughts in the comments. Happy coding!\u00a0\ud83d\ude80</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=47f1c4b7c770\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":[]},{"title":"Transform YouTube Videos into Text and Summaries with AI","pubDate":"2025-02-14 16:32:59","link":"https://medium.com/@mussadiqahmed90/transform-youtube-videos-into-text-and-summaries-with-ai-ca04f33f1866?source=rss-01d818814266------2","guid":"https://medium.com/p/ca04f33f1866","author":"Muhammad Ahmed","thumbnail":"","description":"\n<p>Have you ever wanted to get subtitles or a quick summary of a YouTube video without watching the whole thing? Whether you\u2019re a student, a content creator, or just someone who prefers reading over watching, this AI-powered tool can help you save\u00a0time.</p>\n<h3>What Does This Tool\u00a0Do?</h3>\n<p>This tool allows you\u00a0to:</p>\n<ul>\n<li>\n<strong>Transcribe YouTube videos</strong> into text\u00a0format.</li>\n<li>\n<strong>Generate subtitles</strong> that can be downloaded in\u00a0.srt or\u00a0.txt\u00a0format.</li>\n<li>\n<strong>Summarize</strong> long video transcripts into short, easy-to-read content.</li>\n</ul>\n<p>It\u2019s all done automatically using <strong>AI models like Whisper</strong> for transcription and <strong>a summarization model</strong> for creating concise summaries.</p>\n<h3>How It\u00a0Works</h3>\n<h3>1. Enter a YouTube Video\u00a0URL</h3>\n<p>Simply paste the link of the video you want to transcribe.</p>\n<h3>2. AI Converts Speech to\u00a0Text</h3>\n<p>The AI extracts audio from the video and transcribes it into\u00a0text.</p>\n<h3>3. Download Subtitles</h3>\n<p>The generated text is formatted into subtitles, which you can download as an\u00a0.srt or\u00a0.txt\u00a0file.</p>\n<h3>4. Get a\u00a0Summary</h3>\n<p>If you don\u2019t want to read the full transcript, the AI can summarize it for you in just a few sentences.</p>\n<h3>Why This is\u00a0Useful</h3>\n<ul>\n<li>\n<strong>Save Time</strong>: Get the key points without watching long\u00a0videos.</li>\n<li>\n<strong>Accessibility</strong>: Helps people with hearing difficulties or those who prefer\u00a0reading.</li>\n<li>\n<strong>Content Creation</strong>: Extract quotes and key insights\u00a0easily.</li>\n</ul>\n<h3>How You Can Use\u00a0It</h3>\n<p>This tool is perfect\u00a0for:</p>\n<ul>\n<li>\n<strong>Students</strong> who need lecture summaries.</li>\n<li>\n<strong>Researchers</strong> who want quick insights from interviews.</li>\n<li>\n<strong>Bloggers &amp; Journalists</strong> who need transcripts for content creation.</li>\n<li>\n<strong>Anyone</strong> who wants to read rather than\u00a0watch!</li>\n</ul>\n<h3>Try It\u00a0Now</h3>\n<p>Want to see it in action? Paste a YouTube link and let AI do the\u00a0work!</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=ca04f33f1866\" width=\"1\" height=\"1\" alt=\"\">\n","content":"\n<p>Have you ever wanted to get subtitles or a quick summary of a YouTube video without watching the whole thing? Whether you\u2019re a student, a content creator, or just someone who prefers reading over watching, this AI-powered tool can help you save\u00a0time.</p>\n<h3>What Does This Tool\u00a0Do?</h3>\n<p>This tool allows you\u00a0to:</p>\n<ul>\n<li>\n<strong>Transcribe YouTube videos</strong> into text\u00a0format.</li>\n<li>\n<strong>Generate subtitles</strong> that can be downloaded in\u00a0.srt or\u00a0.txt\u00a0format.</li>\n<li>\n<strong>Summarize</strong> long video transcripts into short, easy-to-read content.</li>\n</ul>\n<p>It\u2019s all done automatically using <strong>AI models like Whisper</strong> for transcription and <strong>a summarization model</strong> for creating concise summaries.</p>\n<h3>How It\u00a0Works</h3>\n<h3>1. Enter a YouTube Video\u00a0URL</h3>\n<p>Simply paste the link of the video you want to transcribe.</p>\n<h3>2. AI Converts Speech to\u00a0Text</h3>\n<p>The AI extracts audio from the video and transcribes it into\u00a0text.</p>\n<h3>3. Download Subtitles</h3>\n<p>The generated text is formatted into subtitles, which you can download as an\u00a0.srt or\u00a0.txt\u00a0file.</p>\n<h3>4. Get a\u00a0Summary</h3>\n<p>If you don\u2019t want to read the full transcript, the AI can summarize it for you in just a few sentences.</p>\n<h3>Why This is\u00a0Useful</h3>\n<ul>\n<li>\n<strong>Save Time</strong>: Get the key points without watching long\u00a0videos.</li>\n<li>\n<strong>Accessibility</strong>: Helps people with hearing difficulties or those who prefer\u00a0reading.</li>\n<li>\n<strong>Content Creation</strong>: Extract quotes and key insights\u00a0easily.</li>\n</ul>\n<h3>How You Can Use\u00a0It</h3>\n<p>This tool is perfect\u00a0for:</p>\n<ul>\n<li>\n<strong>Students</strong> who need lecture summaries.</li>\n<li>\n<strong>Researchers</strong> who want quick insights from interviews.</li>\n<li>\n<strong>Bloggers &amp; Journalists</strong> who need transcripts for content creation.</li>\n<li>\n<strong>Anyone</strong> who wants to read rather than\u00a0watch!</li>\n</ul>\n<h3>Try It\u00a0Now</h3>\n<p>Want to see it in action? Paste a YouTube link and let AI do the\u00a0work!</p>\n<img src=\"https://medium.com/_/stat?event=post.clientViewed&amp;referrerSource=full_rss&amp;postId=ca04f33f1866\" width=\"1\" height=\"1\" alt=\"\">\n","enclosure":{},"categories":[]}]}